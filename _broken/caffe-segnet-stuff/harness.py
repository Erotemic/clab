# -*- coding: utf-8 -*-
"""
Harness to handle training, testing, and evaulation of semantic segmentation
architectures. With varying datasets, architectures, and hyperparameters.
"""
from __future__ import absolute_import, division, print_function
from os.path import join, expanduser, basename, splitext, exists, abspath
import numpy as np
import ubelt as ub
import sys
import cv2
import glob
from pysseg import util
from pysseg.util import gpu_util
from pysseg.util import imutil
from pysseg.util import jsonutil
from pysseg import scoring
from pysseg import models
from pysseg import tasks
from pysseg import inputs
from pysseg import hyperparams
import pysseg.backend.iface_caffe as iface

from pysseg import getLogger
logger = getLogger(__name__)
print = logger.info


class Harness(object):
    def __init__(harn, test_imdir=None, test_gtdir=None,
                 test_weights_fpath=None, workdir='./work',
                 test_model_fpath=None, train_imdir=None, train_gtdir=None,
                 arch='segnet_basic', task=None):

        harn.workdir = abspath(expanduser(workdir))
        harn.arch = arch

        harn.solver_fpath = None

        harn.test_model_fpath = test_model_fpath
        harn.test_weights_fpath = test_weights_fpath
        harn.test_batch_size = 1
        harn.train_batch_size = 1

        harn.test = inputs.Inputs(tag='test')
        harn.train = inputs.Inputs(tag='train')
        harn.train.imdir = train_imdir
        harn.train.gtdir = train_gtdir
        harn.test.imdir = test_imdir
        harn.test.gtdir = test_gtdir

        harn.prepare_workdir()

        harn.gpu_num = gpu_util.find_unused_gpu(min_memory=8000)

        harn.init_pretrained_fpath = None
        harn.test_dpath = None

        default_params = models.default_hyperparams(arch)
        harn.params = hyperparams.HyperParams(**default_params)

        harn.train_hyper_id = None
        harn.train_init_id = None

        harn.test_weights_dpath = None
        harn.test_dump_dpath = None

        harn._snapshot_dpath = None  # overrides the one in solver

        harn.abbrev = 7

        if task is None:
            harn.task = tasks.DivaV1()  # for class labels
        else:
            harn.task = task

    def set_inputs(harn, train, test):
        harn.train = train
        harn.test = test
        if test:
            harn.test.base_dpath = harn.inputs_base
        if train:
            harn.train.base_dpath = harn.inputs_base

    def prepare_workdir(harn):
        # Ensure output directories exist
        harn.workdir = ub.ensuredir(expanduser(harn.workdir))
        harn.archdir = ub.ensuredir((harn.workdir, 'arch', harn.arch))
        harn.inputs_base = ub.ensuredir((harn.workdir, 'inputs'))
        harn.test.base_dpath = harn.inputs_base
        harn.train.base_dpath = harn.inputs_base

    def convert_weights(harn, model_fpath, weights_fpath):
        """
        The autogenerated model may have different layer names even though it
        is the same architecture. This conversion simply forces caffe to apply
        weights correctly.
        """
        from os.path import join, basename
        weight_fname = basename(weights_fpath)
        new_weight_path = join(harn.workdir, weight_fname)
        iface.convert_weights(model_fpath, harn.test_model_fpath,
                              weights_fpath, new_weight_path)
        return new_weight_path

    def prepare_test_model(harn, force=True):
        """
        Creates a caffe prototext that can be used for prediction
        """
        if force or not harn.test_model_fpath:
            harn.test.prepare_input()
            print('[segnet] Preparing model')
            harn.test_base = ub.ensuredir((harn.archdir, 'test'))
            harn.test_dpath = ub.ensuredir(
                (harn.test_base, 'input_' + harn.test.input_id))

            if harn.test.gt_paths is None:
                # use blobs
                input_fpath = None
            else:
                input_fpath = harn.test.input_fpath

            harn.test_model_fpath = models.make_model_file(
                input_fpath, mode='predict', dpath=harn.test_dpath,
                arch=harn.arch,
                modelkw={'batch_size': harn.test_batch_size,
                         'n_classes': len(harn.task.classnames),
                         'ignore_label': harn.task.ignore_label,
                         })

    def prepare_solver(harn, force=False):
        if force or not harn.solver_fpath:
            harn.train.prepare_input()
            harn.train.prepare_gtstats(harn.task)

            gtstats = harn.train.gtstats
            class_weights = gtstats.loss_weight.loc[harn.task.classnames]
            class_weights = class_weights.drop(harn.task.ignore_classnames)
            print(ub.color_text('[segnet] Preparing solver', 'blue'))
            harn.train_base = ub.ensuredir((harn.archdir, 'train'))

            if harn.init_pretrained_fpath is not None:
                if harn.train_init_id is None:
                    harn.train_init_id = util.get_file_hash(harn.init_pretrained_fpath)[:harn.abbrev]
            else:
                harn.train_init_id = None

            harn.train_hyper_id = util.hash_data(harn.params.hyper_id())[:harn.abbrev]

            harn.train_id = '{}_{}_{}_{}'.format(
                harn.train.input_id, harn.arch, harn.train_init_id,
                harn.train_hyper_id)

            print('+=========')
            print('harn.hyper_strid = {!r}'.format(harn.params.hyper_id()))
            print('harn.train_init_id = {!r}'.format(harn.train_init_id))
            print('harn.arch = {!r}'.format(harn.arch))
            print('harn.train_hyper_id = {!r}'.format(harn.train_hyper_id))
            print('harn.train_id = {!r}'.format(harn.train_id))
            print('+=========')

            harn.solver_dpath = ub.ensuredir((
                harn.train_base,
                'input_' + harn.train.input_id, 'solver_{}'.format(harn.train_id)
            ))
            harn.solver_fpath = models.make_solver_file(
                harn.train.input_fpath, arch=harn.arch, dpath=harn.solver_dpath,
                params=harn.params,
                gpu_num=harn.gpu_num,
                modelkw={'batch_size': harn.train_batch_size,
                         'class_weights': class_weights,
                         'n_classes': len(harn.task.classnames),
                         'ignore_label': harn.task.ignore_label,
                         },
            )

    def make_net(harn):
        print('[segnet] initializing caffe.Net')
        from pysseg.backend.find_segnet_caffe import import_segnet_caffe
        caffe = import_segnet_caffe(gpu_num=harn.gpu_num)
        test_weights_fpath = expanduser(harn.test_weights_fpath)
        test_model_fpath = expanduser(harn.test_model_fpath)
        if sys.version_info[0] == 3:
            # Caffe cant handle unicode apparently
            test_model_fpath = test_model_fpath.encode('utf8')
            test_weights_fpath = test_weights_fpath.encode('utf8')
        else:
            test_model_fpath = str(test_model_fpath)
            test_weights_fpath = str(test_weights_fpath)
        net = caffe.Net(test_model_fpath, test_weights_fpath, caffe.TEST)
        return net

    def fit(harn, prevstate_fpath=None, dry=False):
        """
        Runs the caffe train command

        CommandLine:
            export CMAKE_PREFIX_PATH=$HOME/code/fletch/build-py3/install:$CMAKE_PREFIX_PATH
            export LD_LIBRARY_PATH=$HOME/code/fletch/build-py3/install/lib:$LD_LIBRARY_PATH
            export PYTHONPATH=$HOME/code/fletch/build-py3/install/lib/python3.5/site-packages:$PYTHONPATH
            python -m pysseg.harness Harness.fit:0

            NOTE had to do this in fletch/build-py3/install
            ln -s libleveldb.so libleveldb.so.1

            rsync -avpr aretha:.cache/segnet/snapshots/segnet_basic_solver_cjwbntibegrljovvbrtidvmzkzfylvje.prototext_iter_25000* .
            segnet_basic_solver_cjwbntibegrljovvbrtidvmzkzfylvje.prototext_iter_25000.solverstate
        """
        print('[segnet] begin fit')
        from pysseg.backend.find_segnet_caffe import find_segnet_caffe_bin
        assert harn.solver_fpath is not None, 'must prepare solver fist'
        harn.prepare_solver(force=False)
        caffe_bin = find_segnet_caffe_bin()
        args = [caffe_bin, 'train']
        if harn.gpu_num is not None:
            args += ['-gpu', str(harn.gpu_num)]
        args += ['-solver', harn.solver_fpath]
        if prevstate_fpath is not None:
            args += ['-snapshot', prevstate_fpath]
        elif harn.init_pretrained_fpath is not None:
            # Dont give snapshot and weights
            args += ['-weights', harn.init_pretrained_fpath]
        command = ' '.join(args)

        if not dry:
            result = ub.cmd(command, verbose=2)  # NOQA
            # import os
            # os.system(command)
            # result = ub.cmd(command, verbose=2)
            # print('result = {!r}'.format(result))
        else:
            print('Run this command')
            print(command)

    def fit2(harn, prevstate_fpath=None, dry=False):
        from pysseg.backend.find_segnet_caffe import import_segnet_caffe
        from pysseg.backend import iface_caffe as iface
        caffe = import_segnet_caffe(gpu_num=harn.gpu_num)

        harn.prepare_solver()

        solver_info = iface.parse_solver_info(harn.solver_fpath)
        snapshot_iters = solver_info['snapshot']

        # Assuming that the solver .prototxt has already been configured including
        # the corresponding training and testing network definitions (as .prototxt).
        solver = caffe.SGDSolver(harn.solver_fpath)

        pretrained = harn.init_pretrained_fpath

        prev_iter = 0
        if prevstate_fpath is not None:
            print('Restoring State from {}'.format(prevstate_fpath))
            solver.restore(prevstate_fpath)
            prev_iter = iface.snapshot_iterno(prevstate_fpath)
        elif pretrained is not None:
            # https://github.com/BVLC/caffe/issues/3336
            print('Loading pretrained model weights from {}'.format(pretrained))
            solver.net.copy_from(pretrained)

        # net = self.solver.net
        # Do iterations over batches
        # prev = None
        n_steps = solver_info['display']
        bx = prev_iter
        while bx < solver_info['max_iter']:
            # Run until we can produce a snapshot
            info = solver.step(n_steps)
            print('bx = {!r}'.format(bx))
            print('step info = {}'.format(ub.repr2(info)))
            bx += n_steps
            yield bx

            # TODO: interupt and test without memory errors if possible

            # snapshot_models = iface.load_snapshot_weight_paths(solver_info['snapshot_prefix'])
            # train_weights_path = snapshot_models[-1]
            # if prev != train_weights_path:
            #     # After each snapshot generate a testable model
            #     test_weights_fpath = harn.postprocess_training_weights(train_weights_path=train_weights_path)
            #     prev = test_weights_fpath
            #     yield test_weights_fpath
            # else:
            #     logger.warn('WARNING: no snapshot produced')
            # We should now perform a test run before we continue training

    def snapshot_states(harn):
        """
        List of states we can resume training from
        """
        if harn.solver_fpath is None:
            return []
        solver_info = iface.parse_solver_info(harn.solver_fpath)
        # Ensure caffe is configured
        from pysseg.backend.find_segnet_caffe import import_segnet_caffe
        import_segnet_caffe(harn.gpu_num)
        # Get the most recent set of weights
        snapshot_states = iface.load_snapshot_state_paths(
            solver_info['snapshot_prefix'])
        return snapshot_states

    @property
    def snapshot_dpath(harn):
        if harn._snapshot_dpath is None:
            solver_info = iface.parse_solver_info(harn.solver_fpath)
            snapshot_dpath = solver_info['snapshot_prefix']
            import os
            if not os.path.isdir(snapshot_dpath):
                snapshot_dpath = os.path.dirname(snapshot_dpath)
        else:
            snapshot_dpath = harn._snapshot_dpath
        return snapshot_dpath

    def snapshot_weights(harn):
        """
        Returns all snapshot-ed weights trained by this model, in ascending
        order of number of training iterations.
        """
        if harn.solver_fpath is None:
            return []
        # Get the most recent set of weights
        snapshot_weights = iface.load_snapshot_weight_paths(harn.snapshot_dpath)
        return snapshot_weights

    def deploy_trained_for_testing(harn, stride=1):
        """
        Makes the deployable/testable models, and then this function actually
        deploys them to the test directory (via a link file).
        """
        harn.prepare_test_model(force=False)
        for test_weights_fpath in harn.make_testable_weights(stride):
            dname = splitext(basename(test_weights_fpath)[5:])[0]
            test_weights_dpath = ub.ensuredir((harn.test_dpath, dname))
            link_fpath = join(test_weights_dpath, 'test_weights.caffemodel.lnk')
            ub.writeto(link_fpath, test_weights_fpath)
            yield test_weights_fpath

    def make_testable_weights(harn, stride=1):
        """
        Looks at all training snapshots in descending order (so the most recent
        is processed first) and produces a deployable/testable weight file by
        burning in the batch norm statistics.
        """
        snapshot_weights = list(harn.snapshot_weights())[::stride][::-1]
        for train_weights_path in snapshot_weights:
            test_weights_fpath = harn.postprocess_training_weights(train_weights_path=train_weights_path)
            yield test_weights_fpath

    def postprocess_training_weights(harn, train_weights_path=None, force=False):
        """
        Fixes BN params in the weights file for testing

        Ideally this should be run after dumping a snapshot to disk
        (because it requires training data to compute the BN params)

        However, you can run it before testing IF you still know where all the
        training data is.
        """
        solver_info = iface.parse_solver_info(harn.solver_fpath)
        train_model_path = solver_info['train_model_path']

        if train_weights_path is None:
            # Get the most recent set of weights
            snapshot_models = iface.load_snapshot_weight_paths(solver_info['snapshot_prefix'])
            train_weights_path = snapshot_models[-1]

        iterno = iface.snapshot_iterno(train_weights_path)
        test_weight_suffix = ('{}_{:08d}'.format(harn.train_id,  iterno))

        harn.test_weights_dir = ub.ensuredir((
            harn.solver_dpath, 'testable',
        ))

        # Names of the files we are about to write
        test_deploy_fpath = join(
            harn.test_weights_dir, 'deploy_{}.prototxt'.format(test_weight_suffix))
        test_weights_fpath = join(
            harn.test_weights_dir, 'test_weights_{}.caffemodel'.format(test_weight_suffix))
        BN_calc_path = join(
            harn.test_weights_dir,
            '__temp_bn_stats_{}.prototext'.format(test_weight_suffix)
        )

        if exists(test_weights_fpath) and not force:
            # lazy cache
            harn.test_weights_fpath = test_weights_fpath
            return test_weights_fpath

        print(ub.color_text('Postprocessing BN stats', 'blue'))
        print('Building testable weights from train_model_path = {!r}'.format(train_model_path))
        print('Testable weights will be saved to harn.test_weights_fpath = {!r}'.format(harn.test_weights_fpath))

        harn.prepare_workdir()
        from pysseg.backend import batch_norm_stats
        from google.protobuf import text_format
        train_net = batch_norm_stats.make_testable(train_model_path)
        with open(BN_calc_path, 'w') as f:
            f.write(text_format.MessageToString(train_net))

        # use testable net to calculate BN layer stats
        # these must be TRAIN datas
        train_im_paths, train_gt_paths = batch_norm_stats.extract_dataset(train_net)
        # HACK: remove aug
        train_gt_paths = [p for p in train_gt_paths if '_aug' not in basename(p)]
        train_im_paths = [p for p in train_im_paths if '_aug' not in basename(p)]
        # HACK: remove part-scale2
        train_gt_paths = [p for p in train_gt_paths if 'part-scale2' not in basename(p)]
        train_im_paths = [p for p in train_im_paths if 'part-scale2' not in basename(p)]

        # This does a single forward pass on the testing dataset so we can get
        # some stats
        gpu_num = harn.gpu_num
        test_net, test_msg = batch_norm_stats.make_test_files(
            BN_calc_path, train_weights_path, train_im_paths, gpu_num=gpu_num)

        # save deploy prototxt
        # (note deploy is a prediction model that is meant to be used with blob
        # inputs)
        with open(test_deploy_fpath, 'w') as f:
            f.write(text_format.MessageToString(test_msg))

        test_net.save(test_weights_fpath)

        harn.test_weights_fpath = test_weights_fpath
        return test_weights_fpath

    def find_test_weights_dpaths(harn):
        """
        Searches the test directory for all deployed weight files.
        """
        test_weight_dpaths = sorted(glob.glob(join(harn.test_dpath, 'weights_*')))
        return test_weight_dpaths

    def _test_results_fpaths(harn):
        test_weight_dpaths = harn.find_test_weights_dpaths()
        for test_weights_dpath in test_weight_dpaths:
            results_fpath = join(test_weights_dpath, 'results.json')
            if exists(results_fpath):
                yield results_fpath

    def evaulate_all(harn):
        """
        Performs testing on trained snapshots.
        """
        harn.prepare_test_model(force=False)
        test_weight_dpaths = harn.find_test_weights_dpaths()
        for test_weights_dpath in test_weight_dpaths:
            harn.test_weights_dpath = test_weights_dpath
            harn.test_dump_dpath = test_weights_dpath
            link_fpath = join(test_weights_dpath,
                              'test_weights.caffemodel.lnk')
            harn.test_weights_fpath = ub.readfrom(link_fpath)
            # if not exists(join(harn.test_weights_dpath, 'pred')):
            if not exists(join(harn.test_dump_dpath, 'results.json')):
                print('Need to evaluate: harn.test_weights_fpath = {!r}'.format(harn.test_weights_fpath))
                harn.evaluate()

    # def rescore_all(harn):
    #     """
    #     Recomputes test scores
    #     """
    #     test_weight_dpaths = harn.find_test_weights_dpaths()
    #     for test_weights_dpath in test_weight_dpaths:
    #         harn.test_weights_dpath = test_weights_dpath
    #         harn.test_dump_dpath = harn.test_weights_dpath
    #         harn.score()

    def evaluate(harn):
        """
        Uses python caffe interface to test results

        Notes:
            # Download pretrained weights from
            # https://github.com/alexgkendall/SegNet-Tutorial/blob/master/Example_Models/segnet_model_zoo.md
            http://mi.eng.cam.ac.uk/~agk34/resources/SegNet/segnet_basic_camvid.caffemodel
        """
        harn.predict()
        harn.score()

    def predict(harn, have_true=True):
        # Import the right version of caffe
        print(ub.color_text('[segnet] begin prediction', 'blue'))
        harn.prepare_test_model(force=False)
        harn.test.make_dumpsafe_names()
        net = harn.make_net()
        assert harn.test_batch_size == 1
        # have_true = bool(harn.test.gt_paths)

        if not have_true:
            def load_batch_data(bx):
                """
                bx = 0
                """
                offset = bx * harn.test_batch_size
                blob_data = net.blobs['data'].data
                for jx in range(harn.test_batch_size):
                    # push data into the network
                    ix = offset + jx
                    im_hwc = util.imread(harn.test.im_paths[ix])
                    im_hwc = im_hwc[:, :, ::-1]
                    im_chw = np.transpose(im_hwc, (2, 0, 1)).astype(np.float32)
                    blob_data[jx, :, :, :] = im_chw

        n_iter = int(harn.test.n_input / harn.test_batch_size)
        for bx in ub.ProgIter(range(n_iter), label='forward batch', freq=1):
            if not have_true:
                load_batch_data(bx)
            net.forward()
            blobs = net.blobs
            harn.dump_predictions(blobs, bx, have_true=have_true)

    def dump_predictions(harn, blobs, bx, have_true=True):
        """
        Notes:
            data.shape  = (b, nc, h, w)
            label.shape = (b,  1, h, w)
            prob.shape  = (b,  K, h, w)

            Key:
                b    = batch_size     = 1
                nc   = n_channels     = 3
                h    = height         = 360
                w    = width          = 480
                K    = n_class_labels = 11
        """
        offset = (harn.test_batch_size * bx)
        # For each item in the batch
        for jx in range(harn.test_batch_size):

            if 'data' in blobs:
                data = blobs['data'].data[jx]
                orig = np.transpose(data, (1, 2, 0)).astype(np.uint8)

            if have_true:
                if 'label' in blobs:
                    label = blobs['label'].data[jx]
                    true = np.squeeze(label).astype(np.int)

            if 'argmax' in blobs:
                pred = blobs['argmax'].data[jx][0].astype(np.int)
            elif 'prob' in blobs:
                prob = blobs['prob'].data[jx]
                pred = np.squeeze(np.argmax(prob, axis=0))

            outputs = {
                # 'orig': orig,
                # 'true': true,
                'pred': pred,
            }
            if have_true:
                outputs['true'] = true

            COLORS = True
            if COLORS:
                color_pred = harn.task.colorize(pred)
                # outputs['color_true'] = color_true
                outputs['color_pred'] = color_pred
                outputs['blend_pred'] = imutil.overlay_colorized(color_pred, orig)
                if have_true:
                    color_true = harn.task.colorize(true)
                    outputs['blend_true'] = imutil.overlay_colorized(color_true, orig)

            ix = offset + jx

            name = splitext(basename(harn.test.dump_im_names[ix]))[0]
            for key, img in outputs.items():
                dpath = join(harn.test_dump_dpath, key)
                ub.ensuredir(dpath)
                fpath = join(dpath, '{}.png'.format(name))
                cv2.imwrite(fpath, img)

    def score(harn):
        """
        score predictions dumped into the dump prediction dir against the
        current groundtruth.
        """
        # from sklearn.metrics import confusion_matrix
        pred_paths = glob.glob(join(harn.test_dump_dpath, 'pred/*.png'))
        # hack to make gt and pred align, should generate association instead
        harn._score(harn.test.gt_paths, pred_paths)

    def _score(harn, gt_paths, pred_paths, true_task=None):
        """
        Scores predictions, harn.task must correspond to the training task.

        If true_task is specified, then we assume the ground-truth corresponds
        to a different task. The predicted labels are mapped onto this evaulation
        domain, and scores are computed as best as possible.
        """
        pred_task = harn.task

        results = scoring.score_predictions(gt_paths, pred_paths, true_task,
                                            pred_task)

        class_acc, miou, global_acc = ub.take(results, [
            'class_acc', 'global_miou', 'global_acc'])
        print('class_acc = {!r}'.format(class_acc))
        print('miou = {!r}'.format(miou))
        print('global_acc = {!r}'.format(global_acc))

        results_fpath = join(harn.test_dump_dpath, 'results.json')
        jsonutil.write_json(results_fpath, results)
        return results_fpath


if __name__ == '__main__':
    r"""
    CommandLine:
        python -m pysseg.harness
    """
    import ubelt as ub  # NOQA
    ub.doctest_package()
