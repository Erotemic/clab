Our final solution was an ensemble of two models. The first (and primary) model was based on a PyTorch implementation of UNet for semantic segmentation. In addition to using the three RGB channels we used three auxiliary channels: DSM (digital surface model), DTM (digital terrain model) and the difference between the two. The height maps were scale normalized (globally), and we centered these channels based on the median value in each input to the network. In addition to the binary building/non-building output branch we also included an auxillary branch which was trained to predict 3 classes: non-building, outer-building, and inner-building. The inner buildings were computed using a 5x5 erosion of the groundtruth and the outer buildings were the difference between the original and eroded groundtruth. To train and predict on a larger image we chipped each large image into multiple 480x360 subregions with 75\% overlap. We predict probabilities for each chip and use weighted averaging (weighting the center of each chip higher than the surround) to restiched the chip probabilities into full image probabilities. 

To transform probabilities into building instance predictions we make a mask of building “seeds” using all 4-connected-components were the inner building probability map is above a threshold (seed_thresh). We then make a mask of binary “basins” using the  regions where the binary building probabilities are above a threshold (mask_thresh). (Note, outer-building probabilities are unused). We remove all seed instance with fewer than (min_seed_size) pixels. Then we “grow” each seed into the mask region using a watershed algorithm. Finally any buildings with fewer than (min_size) pixels are removed. We use a combination of random and Bayesian search to optimize hyperparameters to the competition criterion on the validation dataset. 

By itself our first model produces a high score (.873), which would also have made ranked in 9th place. However, to slightly boost our score we combined these probabilities with probabilities predicted by a second model. Our second model is similar to UNet, but we replace each “unet block” with a modified dense block inspired by DenseNet.  We have to use a small growth rate (16) and bottleneck multiplier (2)  due to the intense memory requirements. We linearly combine probabilities from both models to obtain a final probability map (i.e alpha * prob1 + (1-alpha) * prob2). We optimize hyperparameters in the same manner as described before, but we also add the blending param (alpha) to the list of optimized params. After a fixed number of operations we take the best params and perform a final linear search over alpha holding all other params constant. This process produces our final models and optimized hyper params. 

In the following images the probability that each pixel is a building building is overlayed as a heatmap. Additionally, building instance are represented with contours. The colors of the contours correspond to the following labels: a blue is a predicted instance, green is ground truth, red is a false negative, and magenta is a false positive. Note that the operating point were optimized for 

